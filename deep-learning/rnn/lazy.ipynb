{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/lazy.ipynb)\n",
        "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "bbf6d0671dafefcc"
      },
      "id": "bbf6d0671dafefcc"
    },
    {
      "cell_type": "markdown",
      "id": "72ea15f3d70547f7",
      "metadata": {
        "collapsed": false,
        "id": "72ea15f3d70547f7"
      },
      "source": [
        "# Training Deep ANNs with lazy upload of data\n",
        "\n",
        "In the previous notebook, we limited the data loaded into memory to avoid memory issues. However, deep models have plenty of parameters to be learned from lots of data. This generates a problem: we need to load the data into memory to train the model, but we cannot load all the data at once.\n",
        "\n",
        "To solve that problem, mini-batches are used to lazily load the data in small chunks. The `fit` method allows us to pass a generator that yields the data in small chunks (mini-batches). In this way, we do not need to load the whole dataset into memory at once. This technique is very important when dealing with large datasets (big data), very common when training deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5f099c66705669f8",
      "metadata": {
        "id": "5f099c66705669f8",
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:30.302781Z",
          "start_time": "2024-11-29T11:35:28.685694Z"
        }
      },
      "outputs": [],
      "source": [
        "# make sure the required packages are installed\n",
        "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow tensorflow-hub --quiet\n",
        "repo='data-science-course'\n",
        "module='deep-learning/rnn'\n",
        "# if running in colab, install the required packages and copy the necessary files\n",
        "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
        "    import os\n",
        "    if not os.path.exists(repo):\n",
        "        !git clone --filter=blob:none --sparse https://github.com/francisco-ortin/data-science-course.git 2>/dev/null\n",
        "        !cd {repo} && git sparse-checkout init --cone && git sparse-checkout set {module}  2>/dev/null\n",
        "    !cp --update {repo}/{module}/*.py . 2>/dev/null\n",
        "    !mkdir -p img data\n",
        "    !mv {repo}/{module}/img/* img/. 2>/dev/null\n",
        "    !mv {repo}/{module}/data/* data/. 2>/dev/null\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c3e8a3ff34b7dc",
      "metadata": {
        "collapsed": false,
        "id": "e7c3e8a3ff34b7dc"
      },
      "source": [
        "## Parameters\n",
        "\n",
        " Now, we use the 100,000 most frequent words (5,000 in the previous example). We use 49,300 reviews for training (1,000 in the previous notebook), lazily loading the batches into memory (applying the ELMo embedding transformation). For test and validation, we keep using 350 reviews each."
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Consider all the words with a frequency higher than this value. The higher, the more memory is needed.\n",
        "vocabulary_size = 100_000\n",
        "# Compute the maximum length of the reviews (for speeding up the training it is better to cut the reviews)\n",
        "max_review_length = 80\n",
        "# Max number of epochs to train the models (we use early stopping)\n",
        "n_epochs = 50\n",
        "# Embedding dimensions (ELMo embeddings have 1024 dimensions)\n",
        "embedding_dim = 1024\n",
        "# Number of sentences for validation and test, the remaining ones (49,000) will be used for training\n",
        "n_sentences_val, n_max_sentences_test = 350, 350"
      ],
      "metadata": {
        "id": "f5f7eb0c67d58ee5",
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:30.310771Z",
          "start_time": "2024-11-29T11:35:30.306174Z"
        }
      },
      "id": "f5f7eb0c67d58ee5",
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset\n",
        "\n",
        "We load the dataset from the Keras API. 350 for testing, 350 for validation, and 49,300 for training."
      ],
      "metadata": {
        "collapsed": false,
        "id": "c50a52ab52105e74"
      },
      "id": "c50a52ab52105e74"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training sequences: 49,300.\n",
            "Validation sequences: 350.\n",
            "Testing sequences: 350.\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_half, y_half) = keras.datasets.imdb.load_data(num_words=vocabulary_size)\n",
        "assert n_sentences_val + n_max_sentences_test <= len(X_half), \"Not enough sentences for validation and test.\"\n",
        "X_test, X_val = X_half[-n_max_sentences_test:], X_half[-(n_sentences_val + n_max_sentences_test):-n_max_sentences_test]\n",
        "y_test, y_val = y_half[-n_max_sentences_test:], y_half[-(n_sentences_val + n_max_sentences_test):-n_max_sentences_test]\n",
        "# concat to X_train the remaining samples not used for validation and test\n",
        "X_train = np.concatenate((X_train, X_half[:-(n_sentences_val + n_max_sentences_test)]), axis=0)\n",
        "y_train = np.concatenate((y_train, y_half[:-(n_sentences_val + n_max_sentences_test)]), axis=0)\n",
        "X_half, y_half = None, None  # free memory\n",
        "\n",
        "print(f\"Training sequences: {len(X_train):,}.\\nValidation sequences: {len(X_val):,}.\\nTesting sequences: {len(X_test):,}.\")"
      ],
      "metadata": {
        "id": "73a4ffd175d95c8b",
        "outputId": "2dd1c12c-303d-4336-8b84-b8aaac81e423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.187061Z",
          "start_time": "2024-11-29T11:35:30.313024Z"
        }
      },
      "id": "73a4ffd175d95c8b",
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest of the loading process is the same as in the previous notebook."
      ],
      "metadata": {
        "collapsed": false,
        "id": "5f82fcb2a4d25d87"
      },
      "id": "5f82fcb2a4d25d87"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "First reviews in training set, with the corresponding labels:\n",
            "Review 1: <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all.\n",
            "Label: 1.\n",
            "Review 2: <START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then.\n",
            "Label: 0.\n",
            "Review 3: <START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had earnt working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how embarrasing this is to watch save yourself an hour a bit of your life.\n",
            "Label: 0.\n",
            "Review 4: <START> the scots excel at storytelling the traditional sort many years after the event i can still see in my mind's eye an elderly lady my friend's mother retelling the battle of culloden she makes the characters come alive her passion is that of an eye witness one to the events on the sodden heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and breadth of scotland as i discussed it with a friend one night in mallaig a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional reservoir fact and fiction blend with aspiration role models warning stories archetypes magic and mystery br br my name is aonghas like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the impenetrable wonder of scotland its rugged mountains shrouded in mists the stuff of legend yet seach'd is rooted in reality this is what gives it its special charm it has a rough beauty and authenticity tempered with some of the finest gaelic singing you will ever hear br br aonghas angus visits his grandfather in hospital shortly before his death he burns with frustration part of him yearns to be in the twenty first century to hang out in glasgow but he is raised on the western shores among a gaelic speaking community br br yet there is a deeper conflict within him he yearns to know the truth the truth behind his grandfather's ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last fateful journey to the summit of one of scotland's most inaccessible mountains can the truth be told or is it all in stories br br in this story about stories we revisit bloody battles poisoned lovers the folklore of old and the sometimes more treacherous folklore of accepted truth in doing so we each connect with angus as he lives the story of his own life br br seachd the inaccessible pinnacle is probably the most honest unpretentious and genuinely beautiful film of scotland ever made like angus i got slightly annoyed with the pretext of hanging stories on more stories but also like angus i forgave this once i saw the 'bigger picture ' forget the box office pastiche of braveheart and its like you might even forego the justly famous dramatisation of the wicker man to see a film that is true to scotland this one is probably unique if you maybe meditate on it deeply enough you might even re evaluate the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced.\n",
            "Label: 1.\n",
            "Review 5: <START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the microwave and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life.\n",
            "Label: 0.\n"
          ]
        }
      ],
      "source": [
        "# Let's print some reviews. We need to convert the integers (token ids) back to words.\n",
        "word_to_index = {word: index+3 for word, index in keras.datasets.imdb.get_word_index().items()}  # word -> integer dictionary\n",
        "# The IMDB dataset reserves the 4 first indices for special tokens <PAD>, <START>, <OOV>, <END>\n",
        "index_to_word = {value: key for key, value in word_to_index.items()}  # integer -> word dictionary\n",
        "index_to_word[0] = \"<PAD>\"\n",
        "index_to_word[1] = \"<START>\"\n",
        "index_to_word[2] = \"<OOV>\"\n",
        "index_to_word[3] = \"<END>\"\n",
        "\n",
        "\n",
        "def decode_review(encoded_review: list[int]) -> str:\n",
        "    \"\"\"Decode a review from a list of integers to a string.\"\"\"\n",
        "    return ' '.join(index_to_word.get(word_index, \"<OOV>\") for word_index in encoded_review)\n",
        "\n",
        "# We show the first reviews and their corresponding sentiment.\n",
        "print(\"First reviews in training set, with the corresponding labels:\")\n",
        "for (i, (review, label)) in enumerate(zip(X_train[:5], y_train[:5])):\n",
        "    print(f\"Review {i + 1}: {decode_review(review)}.\\nLabel: {label}.\")\n",
        "\n",
        "# We add padding to the reviews to have the same length. We use the `post` mode to pad and truncate at the end of the reviews.\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
        "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "c98ebd5c105cfde3",
        "outputId": "6c42de0b-bf89-4945-8a1a-5413b03d2fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.192043Z",
          "start_time": "2024-11-29T11:35:31.192043Z"
        }
      },
      "id": "c98ebd5c105cfde3",
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ELMo embeddings\n",
        "\n",
        "The `get_elmo_embeddings` function returns the ELMo embeddings for a list of sentences. It is used to convert the small test and validation sets to ELMo embeddings. For training, we have to do it lazily, since we cannot load all the data into memory at once."
      ],
      "metadata": {
        "collapsed": false,
        "id": "d564187c851e6ad8"
      },
      "id": "d564187c851e6ad8"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "def get_elmo_embeddings(sentences: list[str]) -> np.array:\n",
        "    \"\"\"\n",
        "    Get ELMo embeddings for a list of sentences.\n",
        "    \"\"\"\n",
        "    # ELMo returns a tensor, but we want to extract the embeddings\n",
        "    embeddings = elmo.signatures['default'](tf.constant(sentences))['elmo']\n",
        "    return embeddings.numpy()  # Convert to numpy array for easier manipulation\n",
        "\n",
        "X_val_elmo = get_elmo_embeddings([decode_review(review) for review in X_val])\n",
        "X_test_elmo = get_elmo_embeddings([decode_review(review) for review in X_test])"
      ],
      "metadata": {
        "id": "43ef1964ad0dad2",
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.195207Z",
          "start_time": "2024-11-29T11:35:31.194204Z"
        }
      },
      "id": "43ef1964ad0dad2",
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lazy data generation for training\n",
        "\n",
        "The following `generate_data_lazy` function generates the training data in a lazy way, batch after batch, to avoid memory issues."
      ],
      "metadata": {
        "collapsed": false,
        "id": "be41a096e9dbefa9"
      },
      "id": "be41a096e9dbefa9"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def generate_data_lazy(X_train_p: np.array, y_train_p: np.array, batch_size: int, n_epochs_p:int) -> np.array:\n",
        "    \"\"\"\n",
        "    Generate training data in a lazy way, batch after batch, to avoid memory issues.\n",
        "    In this way, all the data is not loaded into memory at once.\n",
        "    :param X_train_p: The original training data with original shape (max_sentences_train, max_review_length)\n",
        "    :param y_train_p: The original training labels with original shape (max_sentences_train,)\n",
        "    :param batch_size: The batch size\n",
        "    :param n_epochs_p: The number of epochs\n",
        "    :return: Each batch of data, with ELMo embeddings of shape (batch_size, max_review_length, embedding_dim)\n",
        "    \"\"\"\n",
        "    for epoch in range(n_epochs_p):\n",
        "        to_index = 0\n",
        "        for batch_number in range(X_train_p.shape[0] // batch_size):\n",
        "            from_index, to_index = batch_number*batch_size, (batch_number+1)*batch_size\n",
        "            X_train_elmo_batch = get_elmo_embeddings([decode_review(review) for review in X_train_p[from_index:to_index]])\n",
        "            yield X_train_elmo_batch, np.array(y_train_p[from_index:to_index])\n",
        "        X_train_elmo_batch = get_elmo_embeddings([decode_review(review) for review in X_train_p[to_index:]])\n",
        "        yield X_train_elmo_batch, np.array(y_train_p[to_index:])\n"
      ],
      "metadata": {
        "id": "419ca03571ca5b6d",
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.198752Z",
          "start_time": "2024-11-29T11:35:31.197678Z"
        }
      },
      "id": "419ca03571ca5b6d",
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the `train` function is modified to use the `generate_data_lazy` function to train the model using a lazy data generator. The rest of the parameters are the same as in the previous notebook."
      ],
      "metadata": {
        "collapsed": false,
        "id": "2c21cb3cd6d19e51"
      },
      "id": "2c21cb3cd6d19e51"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def compile_train_evaluate(model_p: keras.Model, X_train_p: np.array, y_train_p: np.array,\n",
        "                           x_val_p: np.array, y_val_p: np.array, x_test_p: np.array, y_test_p: np.array,\n",
        "                           batch_size: int, epochs: int) -> (float, float, keras.Model):\n",
        "    \"\"\"\n",
        "    Compile, train and evaluate the model.\n",
        "    :param model_p: the model to compile, train and evaluate\n",
        "    :param X_train_p: train X sequences\n",
        "    :param y_train_p: train y labels\n",
        "    :param x_val_p: validation X sequences\n",
        "    :param y_val_p: validation y labels\n",
        "    :param x_test_p: test X sequences\n",
        "    :param y_test_p: test y labels\n",
        "    :param batch_size: batch size\n",
        "    :param epochs: number of epochs\n",
        "    :return: (test_loss, test_accuracy, model)\n",
        "    \"\"\"\n",
        "    # we compile and train the model\n",
        "    model_p.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
        "    model_p.fit(generate_data_lazy(X_train_p, y_train_p, batch_size, epochs),\n",
        "                batch_size=batch_size, epochs=epochs,\n",
        "                steps_per_epoch=X_train_p.shape[0] // batch_size + 1,\n",
        "                validation_data=(x_val_p, y_val_p),\n",
        "                callbacks=[early_stopping_callback])\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model_p.evaluate(x_test_p, y_test_p)\n",
        "    return loss, accuracy, model_p"
      ],
      "metadata": {
        "id": "a44817b16d6de233",
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.209781Z",
          "start_time": "2024-11-29T11:35:31.206531Z"
        }
      },
      "id": "a44817b16d6de233",
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model compilation, training, and evaluation\n",
        "\n",
        "We create the same model as in the previous notebook, compile, train, and evaluate it."
      ],
      "metadata": {
        "collapsed": false,
        "id": "46fa539415841cc8"
      },
      "id": "46fa539415841cc8"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m557,568\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">557,568</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m656,513\u001b[0m (2.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">656,513</span> (2.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m656,513\u001b[0m (2.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">656,513</span> (2.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 346ms/step - accuracy: 0.7468 - loss: 0.5060 - val_accuracy: 0.8286 - val_loss: 0.3491\n",
            "Epoch 2/50\n",
            "\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 344ms/step - accuracy: 0.8292 - loss: 0.3746 - val_accuracy: 0.8600 - val_loss: 0.3224\n",
            "Epoch 3/50\n",
            "\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 344ms/step - accuracy: 0.8502 - loss: 0.3394 - val_accuracy: 0.8600 - val_loss: 0.3212\n",
            "Epoch 4/50\n",
            "\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 345ms/step - accuracy: 0.8722 - loss: 0.3006 - val_accuracy: 0.8629 - val_loss: 0.3274\n",
            "Epoch 5/50\n",
            "\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 345ms/step - accuracy: 0.8881 - loss: 0.2632 - val_accuracy: 0.8543 - val_loss: 0.3663\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8435 - loss: 0.3428\n",
            "Test loss: 0.4027.\n",
            "Test accuracy: 0.8086.\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None, embedding_dim), dtype=\"float32\")\n",
        "# Add 2 LSTM layers\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2))(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Dropout layer with 20% rate\n",
        "x = layers.Dropout(0.2)(x)\n",
        "# Add a classifier (sigmoid activation function for binary classification)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "elmo_model = keras.Model(inputs, outputs)\n",
        "elmo_model.summary()\n",
        "\n",
        "# Compile, train and evaluate the model\n",
        "test_loss, test_accuracy, elmo_model = compile_train_evaluate(elmo_model, X_train, y_train, X_val_elmo, y_val, X_test_elmo, y_test, 32, n_epochs)\n",
        "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
      ],
      "metadata": {
        "id": "a10223fcc7637cb3",
        "outputId": "9fd89393-8bd8-4e69-c6b0-ec1849f995b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.214295Z",
          "start_time": "2024-11-29T11:35:31.214295Z"
        }
      },
      "id": "a10223fcc7637cb3",
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Let's test the model with the same reviews as in the previous notebook. Feel free to add more reviews to test the model."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9e238be6c403d09f"
      },
      "id": "9e238be6c403d09f"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
            "Review 1: The movie was a great waste of time. I is awful and boring.\n",
            "Probability of being positive: 0.0121.\n",
            "\n",
            "Review 2: I loved the movie. The plot was amazing.\n",
            "Probability of being positive: 0.8733.\n",
            "\n",
            "Review 3: This movie is not worth watching.\n",
            "Probability of being positive: 0.7271.\n",
            "\n",
            "Review 4: Although the film is not a masterpiece, you may have a good time if your expectations are not high.\n",
            "Probability of being positive: 0.7326.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_reviews = [\"The movie was a great waste of time. I is awful and boring.\",\n",
        "                   \"I loved the movie. The plot was amazing.\",\n",
        "                   \"This movie is not worth watching.\",\n",
        "                   \"Although the film is not a masterpiece, you may have a good time if your expectations are not high.\"]\n",
        "\n",
        "\n",
        "review_embeddings = get_elmo_embeddings(example_reviews)\n",
        "predictions = elmo_model.predict(review_embeddings)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Review {i + 1}: {example_reviews[i]}\")\n",
        "    print(f\"Probability of being positive: {prediction[0]:.4f}.\\n\")"
      ],
      "metadata": {
        "id": "9ef6db743cd17de",
        "outputId": "1513da44-99ec-41e7-fa14-78d8d881d5a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-11-29T11:35:31.216530Z",
          "start_time": "2024-11-29T11:35:31.216530Z"
        }
      },
      "id": "9ef6db743cd17de",
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✨ Questions\n",
        "\n",
        "1. Has this model improved the accuracy of the similar model in the previous notebook?\n",
        "2. What is the main reason that explains the previous answer?\n",
        "3. When do you think you will need to train a model in this way?"
      ],
      "metadata": {
        "collapsed": false,
        "id": "9ef63507e7f6d1f1"
      },
      "id": "9ef63507e7f6d1f1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answers\n",
        "\n",
        "*Write your answers here.*\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "5461946251899efc"
      },
      "id": "5461946251899efc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}