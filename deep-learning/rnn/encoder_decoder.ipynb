{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/encoder_decoder.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder-Decoder architecture\n",
    "\n",
    "The [Encoder-Decoder architecture](https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392) is a neural network architecture used in sequence-to-sequence (Seq2Seq) tasks. It is composed of two main parts: the encoder and the decoder. The encoder processes the input sequence and compresses it into a fixed-size internal representation (hidden state of context vector). The decoder is a conditional language model that generates the output sequence.\n",
    "\n",
    "On of the common usages of the Encoder-Decoder architecture is in [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation), where the input sequence is a sentence in one language and the output sequence is the translation of the sentence in another language. The encoder processes the input sentence and compresses it into a fixed-size internal representation. The decoder generates the translation of the sentence in the target language.\n",
    "\n",
    "In this notebook, we will implement a simple Encoder-Decoder architecture using Recurrent Neural Networks (RNN) to translate English into Spanish. The Enoder is a simple bidirectional LSTM network, and the decoder is a simple LSTM network. \n",
    "\n",
    "<img src=\"img/encoder-decoder.jpg\" width=\"1200\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af819f8c13c3ccfa"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:22.843269Z",
     "start_time": "2024-12-04T08:59:21.132519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow --quiet\n",
    "repo='data-science-course'\n",
    "module='deep-learning/rnn'\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !pip uninstall -y keras --quiet\n",
    "    !pip install keras==2.15.0 --quiet\n",
    "    !pip install tensorflow==2.15.1 --quiet\n",
    "    import os\n",
    "    if not os.path.exists(repo):\n",
    "        !git clone --filter=blob:none --sparse https://github.com/francisco-ortin/data-science-course.git 2>/dev/null\n",
    "        !cd {repo} && git sparse-checkout init --cone && git sparse-checkout set {module}  2>/dev/null\n",
    "    !cp --update {repo}/{module}/*.py . 2>/dev/null\n",
    "    !mkdir -p img data\n",
    "    !mv {repo}/{module}/img/* img/. 2>/dev/null\n",
    "    !mv {repo}/{module}/data/* data/. 2>/dev/null\n",
    "\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important variables\n",
    "\n",
    "We define the following variables:\n",
    "- `vocab_size`: the size of the vocabulary (number of unique words in both English and Spanish languages).\n",
    "- `max_length`: the maximum length of the input and output sequences (in words). If a sequence is longer than this, it will be truncated. If it is shorter, it will be padded.\n",
    "- `chars_to_remove`: a list of characters to remove from the text.\n",
    "- `train_size_percentage`: the percentage of the data to use for training ([0-100]).\n",
    "- `embedding_size`: the size of the embedding layer (hyperparameter).\n",
    "- `n_epochs`: the maximum number of epochs to train the model (early stopping is used).\n",
    "- `SOS_word`, `EOS_word`: the start and end of sentence special words.\n",
    "- `n_lstm_units`: the number of LSTM units in the Encoder and Decoder RNNs.\n",
    "- `model_file_name`: the file name to save or load the trained model. If the file exists, the model is loaded from disk, otherwise, the model is trained and saved."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65cd51ca9dd147d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab_size = 5_000\n",
    "max_length = 50\n",
    "chars_to_remove = [\"¡\", \"¿\"]\n",
    "train_size_percentage = 85\n",
    "embedding_size = 128\n",
    "n_epochs = 10\n",
    "n_lstm_units = 512\n",
    "SOS_word, EOS_word = \"startofsentence\", \"endofsentence\"\n",
    "model_file_name = 'data/english_spanish_encoder_decoder.keras'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:22.857619Z",
     "start_time": "2024-12-04T08:59:22.848703Z"
    }
   },
   "id": "958e5ce1e4e8001c",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data\n",
    "\n",
    "Our dataset is a collection of 118,964 English-Spanish sentence pairs taken from [here](https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip). We load the file and remove special characters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e6ac944f5fef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read the contents of the data/english-spanish.txt file\n",
    "with open(\"data/english-spanish.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# remove the special characters\n",
    "for special_char in chars_to_remove:\n",
    "    text = text.replace(special_char, \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:22.935617Z",
     "start_time": "2024-12-04T08:59:22.861355Z"
    }
   },
   "id": "e14a73f497875d41",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get a list of the English and Spanish sentences by splitting each line by the tab character. We shuffle the list of pairs, convert it to a pair of lists and show some examples in them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5e3f3027b4b20"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 118,964.\n",
      "Some example translations:\n",
      "\t1: The ball rolled into the stream. -> La pelota rodó hasta el arrollo.\n",
      "\t2: You'd better not go. -> Mejor no vayas.\n",
      "\t3: I'm learning English. -> Estoy aprendiendo inglés.\n",
      "\t4: I wouldn't have done that if I were you. -> Si yo fuera tú, no habría hecho eso.\n",
      "\t5: They attempted in vain to bribe the witness. -> Ellos trataron en vano de sobornar al testigo.\n"
     ]
    }
   ],
   "source": [
    "# take the English and Spanish sentences, by splitting each line by the tab character\n",
    "pairs: list[(str, str)] = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "# take a list of pairs and returns a pair of lists: one with the English sentences and one with the Spanish sentences\n",
    "sentences_en, sentences_es = zip(*pairs)\n",
    "\n",
    "assert (n_sentences := len(sentences_en)) == len(sentences_es)\n",
    "print(f\"Number of sentences: {n_sentences:,}.\")\n",
    "\n",
    "print(\"Some example translations:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\t{i+1}: {sentences_en[i]} -> {sentences_es[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:23.217124Z",
     "start_time": "2024-12-04T08:59:22.939275Z"
    }
   },
   "id": "6ed0b0a854205a7d",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Encoder-Decoder ANN has two inputs: one for the Encoder (English) and another one for the Decoder (Spanish). Both are strings. However, we create a `TextVectorization` layer for each input, which transforms a batch of strings into a list of token indices or ids (ints). Upon creation, we pass the vocabulary size and the maximum length of the sequences.  \n",
    "\n",
    "Word index/id definition is performed with the `adapt` method, which transforms each input sentence into a list of word indices, considering the vocabulary size. The most frequent words will be mapped to the first token indices, and the least frequent words to the last token indices. Those with a frequency below the vocabulary size will be mapped to the same token index [UNK].\n",
    "\n",
    "For the Decoder input (Spanish sentences), we include the start and end of sentence tokens (SOS and EOS). SOS will indicate the Decoder to start generating the first Spanish word, and EOS will indicate the end of the sentence (termination of generation)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0c8581c7e4dd658"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some example English words: ['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "Some example Spanish words: ['', '[UNK]', 'startofsentence', 'endofsentence', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization is as keras layer that converts a batch of strings into either a list of token indices / ids (ints)\n",
    "# It could also output a dense representation of the strings, where each token is represented by a dense vector (not used here)\n",
    "# sentences longer than `max_length` are truncated, and shorter sentences are padded with zeros\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "# adapt makes the layer to transform each input sentence into a list of word indices, considering the vocabulary size\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "# we adapt the Spanish  layer to the Spanish sentences, including the start and end of sentence tokens\n",
    "text_vec_layer_es.adapt([f\"{SOS_word} {sentence} {EOS_word}\" for sentence in sentences_es])\n",
    "\n",
    "print(f\"Some example English words: {text_vec_layer_en.get_vocabulary()[:10]}\")  # 0 is padding, visualized as ''\n",
    "print(f\"Some example Spanish words: {text_vec_layer_es.get_vocabulary()[:10]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:37.470246Z",
     "start_time": "2024-12-04T08:59:23.220790Z"
    }
   },
   "id": "da62a9a2f125d58",
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the data into training and validation sets (both for Encoder and Decoder inputs). We include the start of sentence token at the beginning of each sentence. The output of the Decoder is the same as the input, but shifted one position to the right, and with the end of sentence token at the end of each sentence.\n",
    "\n",
    "The input is one full English sentence (e.g., \"I like soccer\") for the Encoder and the corresponding Spanish sentence prefixed with SOS (e.g., \"SOS Me gusta el fútbol\") for the Decoder. The output is the Spanish sentence without SOS and with EOS at the end (e.g., \"Me gusta el fútbol EOS\"). In this way, give \"I like soccer\" to the Encoder, and SOS to the Decoder, the latter will generate \"Me\". This output will be passed again to the Decoder concatenated to the previous input (i.e., \"SOS Me\"), which will generate \"gusta\", and so on, until it generates the EOS."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9eda0d14cb02e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# we split the data into training and validation sets\n",
    "# we first take the input for the Encoder (English sentences)\n",
    "X_train_encoder = tf.constant(sentences_en[:n_sentences * train_size_percentage // 100])\n",
    "X_valid_encoder = tf.constant(sentences_en[n_sentences*train_size_percentage//100:])\n",
    "\n",
    "# then, we take the input for the Decoder (Spanish sentences)\n",
    "# We include the SOS at the beginning of each sentence. This is because we want the Decoder to start generating\n",
    "# the first Spanish word, by passing SOS as the first input. Then, the Decoder will generate the first word and\n",
    "# we will pass it to the Decoder again, so it can generate the second word, and so on, until it generates the EOS.\n",
    "# EOS does not need to be added to the input, since we want the Decoder to generate it (it will be added to\n",
    "# Y training dataset).\n",
    "X_train_decoder = tf.constant([f\"{SOS_word} {sentence}\" for sentence in\n",
    "                               sentences_es[:n_sentences * train_size_percentage // 100]])\n",
    "X_valid_decoder = tf.constant([f\"{SOS_word} {sentence}\" for sentence in\n",
    "                               sentences_es[n_sentences * train_size_percentage // 100:]])\n",
    "\n",
    "# The output of the Decoder is the same as the input, but shifted one position to the right, and with EOS at the end\n",
    "# of each sentence. This is because we want the Decoder to generate the first word of the Spanish sentence, then\n",
    "# the second word, and so on, until it generates the EOS.\n",
    "Y_train = text_vec_layer_es([f\"{sentence} {EOS_word}\" for sentence in sentences_es[:n_sentences * train_size_percentage // 100]])\n",
    "Y_valid = text_vec_layer_es([f\"{sentence} {EOS_word}\" for sentence in sentences_es[n_sentences * train_size_percentage // 100:]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:38.398835Z",
     "start_time": "2024-12-04T08:59:37.473388Z"
    }
   },
   "id": "fe9335f6522bdf48",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the model\n",
    "\n",
    "Let's create the Encoder-Decoder ANN with two recurrent neural networks (RNNs). The Encoder is a bidirectional LSTM network, and the Decoder is a simple LSTM network. The Encoder processes the input sequence and compresses it into a fixed-size internal representation. The Decoder generates the output sequence. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25d3b8c50f945417"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_model(n_lstm_units_p: int, vocab_size_p: int) -> Model:\n",
    "    \"\"\"\n",
    "    Creates a Keras model for the Encoder-Decoder architecture.\n",
    "    :param n_lstm_units_p: Number of LSTM units in the Encoder and Decoder.\n",
    "    :param vocab_size_p: Vocabulary size.\n",
    "    :return: The model\n",
    "    \"\"\"\n",
    "    # Both the Encoder and the Decoder will receive a batch of sentences (strings) as input \n",
    "    # (English sentences for the Encoder, and Spanish sentences for the Decoder).\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "    # We connect the inputs t the text vectorization layers using the Keras functional API\n",
    "    # In this way, the input sequences are converted into lists of word indices / ids using the TextVectorization layers\n",
    "    encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "    decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "    # The word indices are then converted into dense vectors using an Embedding layer of `embedding_size` dimensions\n",
    "    # The padding character zero is masked out, so it is ignored by the model (its weight is not updated/learned). This speeds up training.\n",
    "    encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size_p, embedding_size, mask_zero=True)\n",
    "    decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size_p, embedding_size, mask_zero=True)\n",
    "    # we connect the embedding layers to the input indices (functional API)\n",
    "    encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "    decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "    # We create the Encoder as a single bidirectional LSTM layer with half of the units (two LSTMs, one for each direction)\n",
    "    # Return_state=True => gets a reference to the layer’s final state (not the output for all the RNN steps)\n",
    "    # Since we are using a bi-LSTM layer, the final state is a tuple containing 2 short- and 2 long-term states,\n",
    "    # one pair for each direction (that is why we use *encoder_states, to store the four states in one single tuple variable)\n",
    "    encoder = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(n_lstm_units_p // 2, return_state=True))\n",
    "    encoder_outputs, *encoder_states = encoder(encoder_embeddings)\n",
    "\n",
    "    # we concatenate the states of the left and right LSTMs (first, the 2 short-term states and then the 2 long-term states)\n",
    "    # this way, we get a single state for each type (short and long-term) to be passed \n",
    "    # to the one-directional Decoder RNN as its initial state (conditional language model)\n",
    "    encoder_state = (keras.layers.Concatenate(axis=1)([encoder_states[0], encoder_states[2]]), # short-term (0 & 2)\n",
    "                     keras.layers.Concatenate(axis=1)([encoder_states[1], encoder_states[3]])) # long-term (1 & 3)\n",
    "\n",
    "    # The Decoder is also an LSTM layer with `n_lstm_units` units, but it returns sequences (return_sequences=True)\n",
    "    # instead of the final state: we want to know the output (probabilities) for all the words in the Spanish sentence, not just the last one. \n",
    "    # It cannot be bidirectional, since it needs to generate the words in order (otherwise, it would be cheating).\n",
    "    # Remember that the Decoder is a conditional language model, so it needs to receive the states of the Encoder\n",
    "    # (initial_state parameter)\n",
    "    decoder = tf.keras.layers.LSTM(n_lstm_units_p, return_sequences=True)\n",
    "    decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "    # For each step in the Decoder RNN, we add a Dense layer with a softmax activation function to predict the next word in the Spanish sentence\n",
    "    output_layer = tf.keras.layers.Dense(vocab_size_p, activation=\"softmax\")\n",
    "    Y_probas = output_layer(decoder_outputs)\n",
    "\n",
    "    # Finally, we create the Keras Model, specifying the inputs and outputs\n",
    "    model_loc = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_probas])\n",
    "    \n",
    "    model_loc.summary()\n",
    "    return model_loc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:59:38.418655Z",
     "start_time": "2024-12-04T08:59:38.400111Z"
    }
   },
   "id": "25dbdaba1d1aa498",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compile and train the model. We use the `sparse_categorical_crossentropy` as the loss function, since the targets are integers (word indices / ids). Otherwise, if we had one-hot vectors, we would use `categorical_crossentropy`. We use the Nadam optimizer and accuracy as a metric. We train the model for a maximum of `n_epochs` epochs, using a batch size of 32. We use early stopping with patience of 2 epochs and restore the best weights.\n",
    "\n",
    "If the model is already saved in the file `model_file_name`, we load it from disk. Otherwise, we compile and train the model and save it to disk.\n",
    "\n",
    "*Notice*: if you run the following cell, you need a GPU (otherwise, it will take more than 3 hours to train the model)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a53d8608fbbd0df"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization_8 (Text  (None, 50)                   0         ['input_13[0][0]']            \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)       [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)    (None, 50, 128)              640000    ['text_vectorization_8[0][0]']\n",
      "                                                                                                  \n",
      " text_vectorization_9 (Text  (None, 50)                   0         ['input_14[0][0]']            \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirecti  [(None, 512),                788480    ['embedding_12[0][0]']        \n",
      " onal)                        (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)    (None, 50, 128)              640000    ['text_vectorization_9[0][0]']\n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 512)                  0         ['bidirectional_6[0][1]',     \n",
      " )                                                                   'bidirectional_6[0][3]']     \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 512)                  0         ['bidirectional_6[0][2]',     \n",
      " )                                                                   'bidirectional_6[0][4]']     \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)              (None, 50, 512)              1312768   ['embedding_13[0][0]',        \n",
      "                                                                     'concatenate_8[0][0]',       \n",
      "                                                                     'concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 50, 5000)             2565000   ['lstm_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5946248 (22.68 MB)\n",
      "Trainable params: 5946248 (22.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "3160/3160 [==============================] - 2544s 800ms/step - loss: 3.8673 - accuracy: 0.3438 - val_loss: 2.8816 - val_accuracy: 0.4522\n",
      "Epoch 2/10\n",
      "3160/3160 [==============================] - 4618s 1s/step - loss: 2.3793 - accuracy: 0.5197 - val_loss: 2.1568 - val_accuracy: 0.5576\n",
      "Epoch 3/10\n",
      "3160/3160 [==============================] - 4582s 1s/step - loss: 1.7419 - accuracy: 0.6141 - val_loss: 1.8433 - val_accuracy: 0.6053\n",
      "Epoch 4/10\n",
      "3160/3160 [==============================] - 4565s 1s/step - loss: 1.3659 - accuracy: 0.6769 - val_loss: 1.7005 - val_accuracy: 0.6329\n",
      "Epoch 5/10\n",
      "3160/3160 [==============================] - 4622s 1s/step - loss: 1.1158 - accuracy: 0.7236 - val_loss: 1.6445 - val_accuracy: 0.6454\n",
      "Epoch 6/10\n",
      "3160/3160 [==============================] - 4588s 1s/step - loss: 0.9282 - accuracy: 0.7615 - val_loss: 1.6374 - val_accuracy: 0.6509\n",
      "Epoch 7/10\n",
      "3160/3160 [==============================] - 4475s 1s/step - loss: 0.7803 - accuracy: 0.7941 - val_loss: 1.6576 - val_accuracy: 0.6524\n",
      "Epoch 8/10\n",
      "3160/3160 [==============================] - 4490s 1s/step - loss: 0.6609 - accuracy: 0.8219 - val_loss: 1.6946 - val_accuracy: 0.6526\n"
     ]
    }
   ],
   "source": [
    "def compile_and_train_model(model: Model, X_train_encoder_p: np.array, X_train_decoder_p: np.array,\n",
    "                            Y_train_p: np.array, X_valid_encoder_p: np.array, X_valid_decoder_p: np.array,\n",
    "                            Y_valid_p: np.array, n_epochs_p: int, model_file_name: str) -> Model:\n",
    "    if os.path.exists(model_file_name):\n",
    "        return load_model(model_file_name)\n",
    "    # we compile and train the model with sparse_categorical_crossentropy as the loss function, since the targets are integers\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.fit((X_train_encoder_p, X_train_decoder_p), Y_train_p,\n",
    "          epochs=n_epochs_p, batch_size=32,\n",
    "          validation_data=((X_valid_encoder_p, X_valid_decoder_p), Y_valid_p),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "    model.save(model_file_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(n_lstm_units, vocab_size)\n",
    "model = compile_and_train_model(model, X_train_encoder, X_train_decoder, Y_train, X_valid_encoder,\n",
    "                                X_valid_decoder, Y_valid, n_epochs, model_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T18:34:28.533506Z",
     "start_time": "2024-12-04T08:59:38.422192Z"
    }
   },
   "id": "37c4d582ef4aa4b6",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "We use the model for inference. Now, we use a greedy search strategy to predict the next word in the Spanish sentence. We take the word with the highest probability as the next word. We continue this process until we predict the end of sentence token."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33e3b30400d20f3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone -> hola a todos.\n",
      "how old are you? -> cuántos años tiene.\n",
      "what is your name? -> cómo se llama.\n",
      "where are you from? -> de dónde eres.\n",
      "I like soccer -> me gusta el fútbol.\n",
      "I want you to try to correctly translate this long sentence from English to Spanish -> quiero que [UNK] algún día a traducir de traducir este color de inglés.\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence_en: str) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Spanish, preparing the input for the model and calling the predict method.\n",
    "    :param sentence_en: The English sentence to translate.\n",
    "    :return: The Spanish translation.\n",
    "    \"\"\"\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        # Encoder input: one English sentence (batch size = 1)\n",
    "        X_inf_encoder = np.array([sentence_en])\n",
    "        # Decoder input: SOS + existing translation (empty at the beginning)\n",
    "        X_inf_decoder = np.array([SOS_word + translation])\n",
    "        # We call predict with (Encoder_input, Decoder_input) to get the probabilities of the next word\n",
    "        # we take the first sentence ([0]) and the probabilities idx-th word (returns a list of probabilities for max_length words)\n",
    "        y_probas = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, word_idx]  # probas of the last predicted word\n",
    "        # we take the word id with the highest probability\n",
    "        predicted_word_id = np.argmax(y_probas)\n",
    "        # we get the word from the vocabulary\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == EOS_word:\n",
    "            # we are done when we predict the end of sentence token\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()\n",
    "\n",
    "\n",
    "# we test the translation with some sentences. Feel free to add more sentences to test the model\n",
    "english_sentences = [\"hello everyone\",\n",
    "                     \"how old are you?\",\n",
    "                     \"what is your name?\",\n",
    "                     \"where are you from?\",\n",
    "                     \"I like soccer\",\n",
    "                     \"I want you to try to correctly translate this long sentence from English to Spanish\"]\n",
    "for sentence in english_sentences:\n",
    "    print(f\"{sentence} -> {translate(sentence)}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T18:34:37.925568Z",
     "start_time": "2024-12-04T18:34:28.538599Z"
    }
   },
   "id": "e211e62fe67248ac",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "1. In the training data \"I like\" is mostly translated as \"Me gustan\". However, the model translates \"I like\" as \"Me gusta\". Why is that?\n",
    "2. Is there any sentence that is not translated correctly? \n",
    "3. If so, why do you think that happens?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98289b5ebf0d9e9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52ae6a1632861faa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beam search\n",
    "\n",
    "The previous greedy search strategy is not the best one. I may take a word with the highest probability, but it may not be the best probability for any second word. That is, $P(w_1, w_2)$ may not be as high as $P(w_1', w_2')$. This is the problem of finding a local maximum instead of a global maximum. Performing an exact search is not tractable, since the number of possible sentences grows exponentially with the length of the sentence.\n",
    "\n",
    "We can use the Beam search strategy, which considers the $k$ most probable words at each step. We keep track of the $k$ most probable sequences and continue the process until we predict the end of sentence token. We take the sequence with the highest probability at the end."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "534fd211a92825c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'hello everyone':\n",
      "Top first words: [(-0.9104477, 'hola'), (-1.7676975, '[UNK]'), (-2.3887048, 'y')]\n",
      "Top translations so far: [(-1.1329316, 'hola a'), (-2.2534118, '[UNK] a'), (-3.7593431, '[UNK] todo')]\n",
      "Top translations so far: [(-1.5704861, 'hola a todos'), (-2.7043881, '[UNK] a todos'), (-3.4118323, 'hola a dios')]\n",
      "Top translations so far: [(-1.5736455, 'hola a todos endofsentence'), (-2.7050316, '[UNK] a todos endofsentence'), (-3.4296794, 'hola a dios endofsentence')]\n",
      "Spanish translation: hola a todos.\n",
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'how old are you?':\n",
      "Top first words: [(-0.070351236, 'cuántos'), (-2.970761, 'qué'), (-5.0579257, 'cómo')]\n",
      "Top translations so far: [(-0.20440114, 'cuántos años'), (-2.883246, 'cuántos hermanos'), (-3.056719, 'qué edad')]\n",
      "Top translations so far: [(-1.1800603, 'cuántos años tiene'), (-1.3173417, 'cuántos años tienen'), (-2.270792, 'cuántos años os')]\n",
      "Top translations so far: [(-1.3825771, 'cuántos años tienen endofsentence'), (-1.486619, 'cuántos años tiene endofsentence'), (-2.8464556, 'cuántos años tiene usted')]\n",
      "Top translations so far: [(-1.3825771, 'cuántos años tienen endofsentence'), (-1.486619, 'cuántos años tiene endofsentence'), (-2.8511126, 'cuántos años tiene usted endofsentence')]\n",
      "Spanish translation: cuántos años tienen.\n",
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'what is your name?':\n",
      "Top first words: [(-0.17323093, 'cómo'), (-1.8916433, 'cuál'), (-5.098996, 'qué')]\n",
      "Top translations so far: [(-0.40220213, 'cómo se'), (-2.032268, 'cuál es'), (-2.5445647, 'cómo te')]\n",
      "Top translations so far: [(-0.4904379, 'cómo se llama'), (-2.1086006, 'cuál es tu'), (-2.8200772, 'cómo te llamas')]\n",
      "Top translations so far: [(-0.5309813, 'cómo se llama endofsentence'), (-2.16114, 'cuál es tu nombre'), (-2.8207068, 'cómo te llamas endofsentence')]\n",
      "Top translations so far: [(-0.5309813, 'cómo se llama endofsentence'), (-2.1616933, 'cuál es tu nombre endofsentence'), (-2.8207068, 'cómo te llamas endofsentence')]\n",
      "Spanish translation: cómo se llama.\n",
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'where are you from?':\n",
      "Top first words: [(-0.2887622, 'de'), (-1.7420112, 'dónde'), (-3.211634, 'a')]\n",
      "Top translations so far: [(-0.32265675, 'de dónde'), (-2.8888526, 'dónde están'), (-2.893796, 'dónde de')]\n",
      "Top translations so far: [(-0.7886888, 'de dónde eres'), (-2.8731513, 'de dónde sos'), (-2.9946856, 'de dónde está')]\n",
      "Top translations so far: [(-0.8176069, 'de dónde eres endofsentence'), (-2.9113104, 'de dónde sos endofsentence'), (-3.3350294, 'de dónde está usted')]\n",
      "Top translations so far: [(-0.8176069, 'de dónde eres endofsentence'), (-2.9113104, 'de dónde sos endofsentence'), (-3.4721014, 'de dónde está usted endofsentence')]\n",
      "Spanish translation: de dónde eres.\n",
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'I like soccer':\n",
      "Top first words: [(-0.14558186, 'me'), (-2.9651926, 'a'), (-3.6884766, 'soy')]\n",
      "Top translations so far: [(-0.15768993, 'me gusta'), (-3.6610503, 'a mí'), (-4.3089213, 'soy el')]\n",
      "Top translations so far: [(-0.18289688, 'me gusta el'), (-4.120119, 'me gusta la'), (-4.3316307, 'soy el fútbol')]\n",
      "Top translations so far: [(-0.18426616, 'me gusta el fútbol'), (-4.3379297, 'soy el fútbol endofsentence'), (-4.890296, 'me gusta la fútbol')]\n",
      "Top translations so far: [(-0.18461384, 'me gusta el fútbol endofsentence'), (-4.3379297, 'soy el fútbol endofsentence'), (-4.8903604, 'me gusta la fútbol endofsentence')]\n",
      "Spanish translation: me gusta el fútbol.\n",
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'I want you to try to correctly translate this long sentence from English to Spanish':\n",
      "Top first words: [(-0.02530195, 'quiero'), (-5.1401415, 'te'), (-5.380166, 'quieres')]\n",
      "Top translations so far: [(-0.23511733, 'quiero que'), (-3.226783, 'quiero [UNK]'), (-3.4815614, 'quiero darte')]\n",
      "Top translations so far: [(-0.907236, 'quiero que [UNK]'), (-3.0039353, 'quiero que tengas'), (-3.0115647, 'quiero que intentes')]\n",
      "Top translations so far: [(-3.0547643, 'quiero que [UNK] algún'), (-3.3556457, 'quiero que [UNK] este'), (-3.4335618, 'quiero que intentes [UNK]')]\n",
      "Top translations so far: [(-4.8124504, 'quiero que [UNK] algún día'), (-4.9915123, 'quiero que [UNK] algún test'), (-5.4864154, 'quiero que intentes [UNK] este')]\n",
      "Top translations so far: [(-5.689324, 'quiero que [UNK] algún test de'), (-5.742976, 'quiero que [UNK] algún día a'), (-6.0955124, 'quiero que [UNK] algún test para')]\n",
      "Top translations so far: [(-6.321006, 'quiero que [UNK] algún test para traducir'), (-6.386029, 'quiero que [UNK] algún día a traducir'), (-6.7237763, 'quiero que [UNK] algún test de inglés')]\n",
      "Top translations so far: [(-7.0111566, 'quiero que [UNK] algún test para traducir este'), (-7.241055, 'quiero que [UNK] algún día a traducir de'), (-7.5320916, 'quiero que [UNK] algún test de inglés para')]\n",
      "Top translations so far: [(-8.048291, 'quiero que [UNK] algún test de inglés para traducir'), (-8.432155, 'quiero que [UNK] algún test para traducir este documento'), (-8.43898, 'quiero que [UNK] algún día a traducir de traducir')]\n",
      "Top translations so far: [(-8.436158, 'quiero que [UNK] algún test de inglés para traducir este'), (-9.976447, 'quiero que [UNK] algún día a traducir de traducir este'), (-10.21685, 'quiero que [UNK] algún test para traducir este documento a')]\n",
      "Top translations so far: [(-10.018795, 'quiero que [UNK] algún test de inglés para traducir este documento'), (-10.024419, 'quiero que [UNK] algún test de inglés para traducir este tipo'), (-10.356452, 'quiero que [UNK] algún test de inglés para traducir este color')]\n",
      "Top translations so far: [(-10.045923, 'quiero que [UNK] algún test de inglés para traducir este tipo de'), (-10.782111, 'quiero que [UNK] algún test de inglés para traducir este documento endofsentence'), (-11.275771, 'quiero que [UNK] algún test de inglés para traducir este color endofsentence')]\n",
      "Top translations so far: [(-10.782111, 'quiero que [UNK] algún test de inglés para traducir este documento endofsentence'), (-11.275771, 'quiero que [UNK] algún test de inglés para traducir este color endofsentence'), (-11.39751, 'quiero que [UNK] algún test de inglés para traducir este tipo de plata')]\n",
      "Top translations so far: [(-10.782111, 'quiero que [UNK] algún test de inglés para traducir este documento endofsentence'), (-11.275771, 'quiero que [UNK] algún test de inglés para traducir este color endofsentence'), (-11.4308, 'quiero que [UNK] algún test de inglés para traducir este tipo de plata endofsentence')]\n",
      "Spanish translation: quiero que [UNK] algún test de inglés para traducir este documento.\n"
     ]
    }
   ],
   "source": [
    "def beam_search(sentence_en: str, beam_width: int, verbose=False) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Spanish using beam search wit k=beam_width.\n",
    "    :param sentence_en: The Encoder input (English sentence).\n",
    "    :param beam_width: The k parameter of the beam search algorithm.\n",
    "    :param verbose: Whether to display the top words and translations at each step.\n",
    "    :return: The Spanish translation.\n",
    "    \"\"\"\n",
    "    # Translation of the first word\n",
    "    # Encoder input: one English sentence (batch size = 1)\n",
    "    X_inf_encoder = np.array([sentence_en])\n",
    "    # Decoder input: SOS\n",
    "    X_inf_decoder = np.array([SOS_word])\n",
    "    # Predict the probabilities of the first word\n",
    "    y_proba = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, 0]  # first token's probas\n",
    "    # we take the top k words with the highest probabilities Dict{word_id: proba}\n",
    "    top_k_words = tf.math.top_k(y_proba, k=beam_width)\n",
    "    # list of best (log_proba, translation) pairs\n",
    "    # Important: instead of taking Prob(w1) * Prob(w2) * ... * Prob(wn), we take the log of the product:\n",
    "    # log(Prob(w1)) + log(Prob(w2)) + ... + log(Prob(wn))\n",
    "    # this is because the product of many probabilities between 0 and 1 can be very small and lead to 0.0 after some iterations\n",
    "    top_translations = [\n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k_words.values, top_k_words.indices)\n",
    "    ]\n",
    "\n",
    "    # displays the top first words if verbose mode\n",
    "    print(\"Top first words:\", top_translations) if verbose else None\n",
    "\n",
    "    # Translation of the next words (from 1 on)\n",
    "    for idx in range(1, max_length):\n",
    "        # list of best (log_proba, translation) pairs\n",
    "        candidates: list[(float, str)] = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(EOS_word):\n",
    "                candidates.append((log_proba, translation))\n",
    "                # translation is finished, so don't try to extend it\n",
    "                continue\n",
    "            # Encoder input: one English sentence (batch size = 1)\n",
    "            X_inf_encoder = np.array([sentence_en])  # encoder input\n",
    "            # Decoder input: SOS + existing translation\n",
    "            X_inf_decoder = np.array([SOS_word + \" \" + translation])  # decoder input\n",
    "            # probabilites of the new word\n",
    "            y_proba = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, idx]  # last token's proba\n",
    "            # we include in candidates the top k existing translations with all the possible next words and their probabilities\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append((log_proba + np.log(word_proba), f\"{translation} {word}\"))\n",
    "        # we sort the candidates by the log of the probabilities and take the top k\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # displays the top translation so far, if verbose mode\n",
    "        print(\"Top translations so far:\", top_translations) if verbose else None\n",
    "\n",
    "        # the process terminates when all the K top translations end with the EOS token\n",
    "        if all([top_translation.endswith(EOS_word) for _, top_translation in top_translations]):\n",
    "            # returns the best translation pair ([0] because it is sorted by log probabilities),\n",
    "            # take the translation text ([1]) and remove the EOS token\n",
    "            return top_translations[0][1].replace(EOS_word, \"\").strip()\n",
    "\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Translation with beam search for: \\n\\t '{sentence}':\")\n",
    "    translation = beam_search(sentence, 3, verbose=True)\n",
    "    print(f\"Spanish translation: {translation}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T19:17:43.682468Z",
     "start_time": "2024-12-04T18:34:37.927781Z"
    }
   },
   "id": "18013c05d8de4904",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "This simple model performs decently on short sentences, but it struggles with longer sentences. It is possible to significantly improve the translation quality by using [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)). A more sophisticated implementation of the Encoder-Decoder architecture with attention called [Transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) is the state-of-the-art model for machine translation, currently used by GPT, BERT, and many other models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "969d8f28560f892a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "4. What would happen in beam search if we used probability product instead of the sum of $log$ probabilities?\n",
    "5. Do you think the last sentence will be translated better with $k$=10? "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21d3d1b2ff382850"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "\n",
    "*Write your answers here.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73be660cdce9ff09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
