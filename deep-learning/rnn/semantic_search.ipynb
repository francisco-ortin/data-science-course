{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/semantic_search.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "87540b40e3958a06"
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic search\n",
    "\n",
    "In this notebook, we use sentence embeddings to perform [semantic search](https://en.wikipedia.org/wiki/Semantic_search). Semantic search can be defined as the process of retrieving the most relevant contexts based on a query, even if the query and the contexts are not exactly the same. This is different from keyword-based search, where the search engine looks for exact matches of the keywords in the query.\n",
    "\n",
    "We utilize the [Universal Sentence Encoder](https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder) (USE) model by Google to encode both questions and contexts/answers (paragraphs) as embeddings. USE is a pre-trained model that generates semantic embeddings for small to medium-sized text inputs. Then, using a question/answer dataset, we perform semantic search to retrieve the most relevant contexts based on a query. We search for similarity between the encodings of the query and both the contexts and questions, and return the most relevant ones.\n",
    "\n",
    "*Notice*: USE is a relatively lightweight model that provides good performance. For better embeddings but slower performance, transformer-based models like [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) (Bidirectional encoder representations from transformers) can be used.\n",
    "\n",
    "\n",
    "We use the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Stanford Question Answering Dataset) dataset. SQuAD is one of the most widely used datasets for question-answering tasks. It consists of questions posed by crowdworkers on Wikipedia articles, with the corresponding answers as text spans within the articles."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af819f8c13c3ccfa"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "ExecuteTime": {
     "end_time": "2024-11-11T15:58:23.540071Z",
     "start_time": "2024-11-11T15:58:11.445187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow tensorflow-hub datasets --quiet\n",
    "repo='data-science-course'\n",
    "module='deep-learning/rnn'\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    import os\n",
    "    if not os.path.exists(repo):\n",
    "        !git clone --filter=blob:none --sparse https://github.com/francisco-ortin/data-science-course.git 2>/dev/null\n",
    "        !cd {repo} && git sparse-checkout init --cone && git sparse-checkout set {module}  2>/dev/null\n",
    "    !cp --update {repo}/{module}/*.py . 2>/dev/null\n",
    "    !mkdir -p img data\n",
    "    !mv {repo}/{module}/img/* img/. 2>/dev/null\n",
    "    !mv {repo}/{module}/data/* data/. 2>/dev/null\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset and the model\n",
    "\n",
    "We load the SQuAD question-answering dataset and the Universal Sentence Encoder (USE) model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65cd51ca9dd147d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the SQuAD dataset (using only the 'train' split for demonstration)\n",
    "squad_dataset = load_dataset(\"squad\", split=\"train\")\n",
    "\n",
    "# Load the USE model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T15:58:38.481549Z",
     "start_time": "2024-11-11T15:58:23.541450Z"
    }
   },
   "id": "958e5ce1e4e8001c",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extract the contexts (paragraphs) and questions from the dataset. We then display the first few entries for reference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4643b98ddbf7fd86"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "--------------------------------------------------\n",
      "Question: What is in front of the Notre Dame Main Building?\n",
      "\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "--------------------------------------------------\n",
      "Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "--------------------------------------------------\n",
      "Question: What is the Grotto at Notre Dame?\n",
      "\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "--------------------------------------------------\n",
      "Question: What sits on top of the Main Building at Notre Dame?\n",
      "\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract contexts (paragraphs) and corresponding questions\n",
    "contexts = squad_dataset[\"context\"]\n",
    "questions = squad_dataset[\"question\"]\n",
    "\n",
    "# Display the first few entries for reference\n",
    "for i in range(5):\n",
    "    print(f\"Question: {questions[i]}\\n\")    \n",
    "    print(f\"Context: {contexts[i]}\")\n",
    "    print(\"-\" * 50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T15:59:51.571043Z",
     "start_time": "2024-11-11T15:59:51.297736Z"
    }
   },
   "id": "b045fd7083293dc7",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantic search function\n",
    "\n",
    "The following `semantic_search` function performs semantic search to retrieve the most relevant contexts based on a query. A query (`query_p`) parameter is provided. If the embeddings of the questions in the dataset are provided, the function searches the most similar questions, but comparing their embeddings with the one obtained for the query. Otherwise, it searches the most similar contexts (comparing their embeddings with the one for the query). Embedding similarity is computed with cosine similarities, retriving the `top_k` most similar results. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e6ac944f5fef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def semantic_search(query_p: str, contexts_p: np.array, questions_p: np.array,\n",
    "                    top_k: int, questions_embeddings_p: np.array, context_embeddings_p: np.array) \\\n",
    "        -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    This function performs semantic search to retrieve the most relevant contexts based on a query.\n",
    "    If questions_embeddings_p is provided, it searches by questions.\n",
    "    Otherwise, it searches by contexts.\n",
    "    :param query_p: the query to be semantically searched\n",
    "    :param contexts_p: the contexts (paragraphs) to search from\n",
    "    :param questions_p: the questions to search from\n",
    "    :param top_k: how many results to return\n",
    "    :param questions_embeddings_p: the embeddings of the questions (if searching by questions; otherwise None)\n",
    "    :param context_embeddings_p: the embeddings of the contexts (if searching by contexts; otherwise None)\n",
    "    :return: a list of dictionaries containing the most relevant contexts ('context'),\n",
    "    sample questions ('sample_question'), and similarity scores ('similarity')\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = embed([query_p])\n",
    "    # this function allow searching by question or by context\n",
    "    semantic_embeddings = questions_embeddings_p if questions_embeddings_p is not None else context_embeddings_p\n",
    "    # Compute cosine similarities between the query and all queries embeddings\n",
    "    similarities = cosine_similarity(query_embedding, semantic_embeddings).flatten()\n",
    "    # Get the indices of the top_k most similar contexts\n",
    "    # argsort returns the indices that would sort an array ascending, [-top_k:] gets the top_k largest values,\n",
    "    # and [::-1] reverses the order\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    # Retrieve the most similar contexts and their corresponding questions\n",
    "    results = []\n",
    "    for idx in top_k_indices:\n",
    "        results.append({\n",
    "            'context': contexts_p[idx],\n",
    "            'sample_question': questions_p[idx],  # A sample question related to this context\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:03:25.411279Z",
     "start_time": "2024-11-11T16:03:25.404888Z"
    }
   },
   "id": "e14a73f497875d41",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Queries\n",
    "\n",
    "We define some example queries to test the semantic search function. We then encode the contexts (`contexts_embeddings`) and questions (`questions_embeddings`) using the Universal Sentence Encoder (USE). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f47c36bbf890ac04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Example queries\n",
    "queries = ['What is the capital of the United States of America?',\n",
    "           'What language is spoken in Andorra?',\n",
    "           'Who was Martin Luther King?']\n",
    "\n",
    "# Encode the contexts (paragraphs) and questions. This may take some time.\n",
    "questions_embeddings = embed(questions)\n",
    "contexts_embeddings = embed(contexts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:06:08.274987Z",
     "start_time": "2024-11-11T16:04:55.978943Z"
    }
   },
   "id": "8d621eb2871d144b",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show the results\n",
    "\n",
    "We show the results by comparing the query with the questions and the contexts. We display the top 3 most relevant results for each query. In a professional semantic query system, we would use vector databased to compare the similarity of the embeddings in the persistence system. [Retrieval-Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) (RAG) models follow this approach to perform semantic search using [LLMs](https://en.wikipedia.org/wiki/Large_language_model) (Large Language Models) like BERT, GPT or Gemini.\n",
    "\n",
    "The results are pretty good, considering the simplicity of the USE model. For more complex tasks, transformer-based models like [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) can be used, but they are slower and require more computational resources."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfbc5ee97f2496b4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of the United States of America?\n",
      "Question-based search:\n",
      "\tResult 1\n",
      "\tContext: The capital city, Washington, District of Columbia, is a federal district located on land donated by the state of Maryland. (Virginia had also donated land, but it was returned in 1849.) The United States also has overseas territories with varying levels of independence and organization: in the Caribbean the territories of Puerto Rico and the U.S. Virgin Islands, and in the Pacific the inhabited territories of Guam, American Samoa, and the Northern Mariana Islands, along with a number of uninhabited island territories.\n",
      "\tSample Question: What is the capital city of the US?\n",
      "\tSimilarity Score: 0.9184 \n",
      "\n",
      "\tResult 2\n",
      "\tContext: From 1981 to 2010, the average annual precipitation measured at Seattle–Tacoma International Airport was 37.49 inches (952 mm). Annual precipitation has ranged from 23.78 in (604 mm) in 1952 to 55.14 in (1,401 mm) in 1950; for water year (October 1 – September 30) precipitation, the range is 23.16 in (588 mm) in 1976–77 to 51.82 in (1,316 mm) in 1996–97. Due to local variations in microclimate, Seattle also receives significantly lower precipitation than some other locations west of the Cascades. Around 80 mi (129 km) to the west, the Hoh Rain Forest in Olympic National Park on the western flank of the Olympic Mountains receives an annual average precipitation of 142 in (3.61 m). Sixty miles to the south of Seattle, the state capital Olympia, which is out of the Olympic Mountains' rain shadow, receives an annual average precipitation of 50 in (1,270 mm). The city of Bremerton, about 15 mi (24 km) west of downtown Seattle, receives 56.4 in (1,430 mm) of precipitation annually.\n",
      "\tSample Question: What is the capital of the state of Washington?\n",
      "\tSimilarity Score: 0.8114 \n",
      "\n",
      "\tResult 3\n",
      "\tContext: Boston (pronounced i/ˈbɒstən/) is the capital and largest city of the Commonwealth of Massachusetts in the United States. Boston also served as the historic county seat of Suffolk County until Massachusetts disbanded county government in 1999. The city proper covers 48 square miles (124 km2) with an estimated population of 655,884 in 2014, making it the largest city in New England and the 24th largest city in the United States. The city is the economic and cultural anchor of a substantially larger metropolitan area called Greater Boston, home to 4.7 million people and the tenth-largest metropolitan statistical area in the country. Greater Boston as a commuting region is home to 8.1 million people, making it the sixth-largest combined statistical area in the United States.\n",
      "\tSample Question: What is the capital of Massachusetts?\n",
      "\tSimilarity Score: 0.7757 \n",
      "\n",
      "Question-based search:\n",
      "\tResult 1\n",
      "\tContext: In 1785, the assembly of the Congress of the Confederation made New York the national capital shortly after the war. New York was the last capital of the U.S. under the Articles of Confederation and the first capital under the Constitution of the United States. In 1789, the first President of the United States, George Washington, was inaugurated; the first United States Congress and the Supreme Court of the United States each assembled for the first time, and the United States Bill of Rights was drafted, all at Federal Hall on Wall Street. By 1790, New York had surpassed Philadelphia as the largest city in the United States.\n",
      "\tSample Question: On what street did the writing of the Bill of Rights occur?\n",
      "\tSimilarity Score: 0.4757 \n",
      "\n",
      "\tResult 2\n",
      "\tContext: In 1785, the assembly of the Congress of the Confederation made New York the national capital shortly after the war. New York was the last capital of the U.S. under the Articles of Confederation and the first capital under the Constitution of the United States. In 1789, the first President of the United States, George Washington, was inaugurated; the first United States Congress and the Supreme Court of the United States each assembled for the first time, and the United States Bill of Rights was drafted, all at Federal Hall on Wall Street. By 1790, New York had surpassed Philadelphia as the largest city in the United States.\n",
      "\tSample Question: In what building did the Supreme Court of the United States first sit?\n",
      "\tSimilarity Score: 0.4757 \n",
      "\n",
      "\tResult 3\n",
      "\tContext: In 1785, the assembly of the Congress of the Confederation made New York the national capital shortly after the war. New York was the last capital of the U.S. under the Articles of Confederation and the first capital under the Constitution of the United States. In 1789, the first President of the United States, George Washington, was inaugurated; the first United States Congress and the Supreme Court of the United States each assembled for the first time, and the United States Bill of Rights was drafted, all at Federal Hall on Wall Street. By 1790, New York had surpassed Philadelphia as the largest city in the United States.\n",
      "\tSample Question: In what year did New York become the United States capital?\n",
      "\tSimilarity Score: 0.4757 \n",
      "\n",
      "--------------------------------------------------\n",
      "Query: What language is spoken in Andorra?\n",
      "Question-based search:\n",
      "\tResult 1\n",
      "\tContext: The most widely spoken family of languages in southern Europe are the Romance languages, the heirs of Latin, which have spread from the Italian peninsula, and are emblematic of Southwestern Europe. (See the Latin Arch.) By far the most common romance languages in Southern Europe are: Italian, which is spoken by over 50 million people in Italy, San Marino, and the Vatican; and Spanish, which is spoken by over 40 million people in Spain and Gibraltar. Other common romance languages include: Romanian, which is spoken in Romania and Moldova; Portuguese, which is spoken in Portugal; Catalan, which is spoken in eastern Spain; and Galician, which is spoken in northwestern Spain.\n",
      "\tSample Question: What language is spoken in northwest Spain?\n",
      "\tSimilarity Score: 0.8099 \n",
      "\n",
      "\tResult 2\n",
      "\tContext: Although the United States has no de jure official language, English is the dominant language of business, education, government, religion, media, culture, civil society, and the public sphere. Virtually all state and federal government agencies and large corporations use English as their internal working language, especially at the management level. Some states, such as New Mexico, provide bilingual legislated notices and official documents, in Spanish and English, and other commonly used languages. By 2015, there was a trend that most Americans and American residents who are of Hispanic descent speak only English in the home.\n",
      "\tSample Question: What language is spoken in the U.S.A?\n",
      "\tSimilarity Score: 0.8071 \n",
      "\n",
      "\tResult 3\n",
      "\tContext: In Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several Andorranization policies have been enforced, like Catalan medium education.\n",
      "\tSample Question: What is the only language of Andorra?\n",
      "\tSimilarity Score: 0.7813 \n",
      "\n",
      "Question-based search:\n",
      "\tResult 1\n",
      "\tContext: There are other language groupings in Southern Europe. Albanian is spoken in Albania, Kosovo, Macedoonia, and parts of Greece. Maltese is a Semitic language that is the official language of Malta. The Basque language is spoken in the Basque Country, a region in northern Spain and southwestern France.\n",
      "\tSample Question: Where is Basque Country located?\n",
      "\tSimilarity Score: 0.4831 \n",
      "\n",
      "\tResult 2\n",
      "\tContext: There are other language groupings in Southern Europe. Albanian is spoken in Albania, Kosovo, Macedoonia, and parts of Greece. Maltese is a Semitic language that is the official language of Malta. The Basque language is spoken in the Basque Country, a region in northern Spain and southwestern France.\n",
      "\tSample Question: What type of language is Maltese?\n",
      "\tSimilarity Score: 0.4831 \n",
      "\n",
      "\tResult 3\n",
      "\tContext: There are other language groupings in Southern Europe. Albanian is spoken in Albania, Kosovo, Macedoonia, and parts of Greece. Maltese is a Semitic language that is the official language of Malta. The Basque language is spoken in the Basque Country, a region in northern Spain and southwestern France.\n",
      "\tSample Question: What language can be found used in Kosovo and Albania?\n",
      "\tSimilarity Score: 0.4831 \n",
      "\n",
      "--------------------------------------------------\n",
      "Query: Who was Martin Luther King?\n",
      "Question-based search:\n",
      "\tResult 1\n",
      "\tContext: By the 1900s, nigger had become a pejorative word in the United States. In its stead, the term colored became the mainstream alternative to negro and its derived terms. After the African-American Civil rights movement, the terms colored and negro gave way to \"black\". Negro had superseded colored as the most polite word for African Americans at a time when black was considered more offensive. This term was accepted as normal, including by people classified as Negroes, until the later Civil Rights movement in the late 1960s. One well-known example is the identification by Reverend Martin Luther King, Jr. of his own race as \"Negro\" in his famous speech of 1963, I Have a Dream. During the American Civil Rights movement of the 1950s and 1960s, some African-American leaders in the United States, notably Malcolm X, objected to the word Negro because they associated it with the long history of slavery, segregation, and discrimination that treated African Americans as second-class citizens, or worse. Malcolm X preferred Black to Negro, but later gradually abandoned that as well for Afro-American after leaving the Nation of Islam.\n",
      "\tSample Question: Who led the Civil Rights movement?\n",
      "\tSimilarity Score: 0.7061 \n",
      "\n",
      "\tResult 2\n",
      "\tContext: The FBI frequently investigated Martin Luther King, Jr. In the mid-1960s, King began publicly criticizing the Bureau for giving insufficient attention to the use of terrorism by white supremacists. Hoover responded by publicly calling King the most \"notorious liar\" in the United States. In his 1991 memoir, Washington Post journalist Carl Rowan asserted that the FBI had sent at least one anonymous letter to King encouraging him to commit suicide. Historian Taylor Branch documents an anonymous November 1964 \"suicide package\" sent by the Bureau that combined a letter to the civil rights leader telling him \"You are done. There is only one way out for you...\" with audio recordings of King's sexual indiscretions.\n",
      "\tSample Question: Who did MLK begin criticizing in the mid-1960s?\n",
      "\tSimilarity Score: 0.6378 \n",
      "\n",
      "\tResult 3\n",
      "\tContext: The FBI frequently investigated Martin Luther King, Jr. In the mid-1960s, King began publicly criticizing the Bureau for giving insufficient attention to the use of terrorism by white supremacists. Hoover responded by publicly calling King the most \"notorious liar\" in the United States. In his 1991 memoir, Washington Post journalist Carl Rowan asserted that the FBI had sent at least one anonymous letter to King encouraging him to commit suicide. Historian Taylor Branch documents an anonymous November 1964 \"suicide package\" sent by the Bureau that combined a letter to the civil rights leader telling him \"You are done. There is only one way out for you...\" with audio recordings of King's sexual indiscretions.\n",
      "\tSample Question: Did the FBI investigate Martin Luther King Jr.?\n",
      "\tSimilarity Score: 0.6071 \n",
      "\n",
      "Context-based search:\n",
      "\tResult 1\n",
      "\tContext: By the 1900s, nigger had become a pejorative word in the United States. In its stead, the term colored became the mainstream alternative to negro and its derived terms. After the African-American Civil rights movement, the terms colored and negro gave way to \"black\". Negro had superseded colored as the most polite word for African Americans at a time when black was considered more offensive. This term was accepted as normal, including by people classified as Negroes, until the later Civil Rights movement in the late 1960s. One well-known example is the identification by Reverend Martin Luther King, Jr. of his own race as \"Negro\" in his famous speech of 1963, I Have a Dream. During the American Civil Rights movement of the 1950s and 1960s, some African-American leaders in the United States, notably Malcolm X, objected to the word Negro because they associated it with the long history of slavery, segregation, and discrimination that treated African Americans as second-class citizens, or worse. Malcolm X preferred Black to Negro, but later gradually abandoned that as well for Afro-American after leaving the Nation of Islam.\n",
      "\tSample Question: What term replaced negro as mainstream?\n",
      "\tSimilarity Score: 0.4412 \n",
      "\n",
      "\tResult 2\n",
      "\tContext: By the 1900s, nigger had become a pejorative word in the United States. In its stead, the term colored became the mainstream alternative to negro and its derived terms. After the African-American Civil rights movement, the terms colored and negro gave way to \"black\". Negro had superseded colored as the most polite word for African Americans at a time when black was considered more offensive. This term was accepted as normal, including by people classified as Negroes, until the later Civil Rights movement in the late 1960s. One well-known example is the identification by Reverend Martin Luther King, Jr. of his own race as \"Negro\" in his famous speech of 1963, I Have a Dream. During the American Civil Rights movement of the 1950s and 1960s, some African-American leaders in the United States, notably Malcolm X, objected to the word Negro because they associated it with the long history of slavery, segregation, and discrimination that treated African Americans as second-class citizens, or worse. Malcolm X preferred Black to Negro, but later gradually abandoned that as well for Afro-American after leaving the Nation of Islam.\n",
      "\tSample Question: What term followed \"negro\" and \"colored\"?\n",
      "\tSimilarity Score: 0.4412 \n",
      "\n",
      "\tResult 3\n",
      "\tContext: By the 1900s, nigger had become a pejorative word in the United States. In its stead, the term colored became the mainstream alternative to negro and its derived terms. After the African-American Civil rights movement, the terms colored and negro gave way to \"black\". Negro had superseded colored as the most polite word for African Americans at a time when black was considered more offensive. This term was accepted as normal, including by people classified as Negroes, until the later Civil Rights movement in the late 1960s. One well-known example is the identification by Reverend Martin Luther King, Jr. of his own race as \"Negro\" in his famous speech of 1963, I Have a Dream. During the American Civil Rights movement of the 1950s and 1960s, some African-American leaders in the United States, notably Malcolm X, objected to the word Negro because they associated it with the long history of slavery, segregation, and discrimination that treated African Americans as second-class citizens, or worse. Malcolm X preferred Black to Negro, but later gradually abandoned that as well for Afro-American after leaving the Nation of Islam.\n",
      "\tSample Question: What movement sprouted this change in rhetoric? \n",
      "\tSimilarity Score: 0.4412 \n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def show_results(search_results_p: list[dict[str, str]]) -> None:\n",
    "    \"\"\"\n",
    "    Display the search results\n",
    "    :param search_results_p: the search results to display\n",
    "    \"\"\"\n",
    "    for idx, result in enumerate(search_results_p):\n",
    "        print(f\"\\tResult {idx + 1}\")\n",
    "        print(f\"\\tContext: {result['context']}\")\n",
    "        print(f\"\\tSample Question: {result['sample_question']}\")\n",
    "        print(f\"\\tSimilarity Score: {result['similarity']:.4f}\", \"\\n\")\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Question-based search:\")\n",
    "    # Get top 3 most relevant\n",
    "    search_results = semantic_search(query, contexts, questions, 3, questions_embeddings, None)\n",
    "    show_results(search_results)\n",
    "    print(\"Context-based search:\")\n",
    "    # Get top 3 most relevant\n",
    "    search_results = semantic_search(query, contexts, questions, 3, None, contexts_embeddings)\n",
    "    # Display the results\n",
    "    show_results(search_results)\n",
    "    print(\"-\" * 50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:07:23.318394Z",
     "start_time": "2024-11-11T16:07:22.064446Z"
    }
   },
   "id": "6dcb0d49c6b101ae",
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
